"""
ç»Ÿä¸€çš„AIè°ƒç”¨æ¥å£
æ”¯æŒè‡ªåŠ¨æ ¼å¼åŒ–è¯·æ±‚ä»¥é€‚é…ä¸åŒçš„æ¨¡å‹API
"""

from typing import Dict, List, Optional, Any
import openai
import anthropic

from ..models import User
from ..utils.security import decrypt
from ..utils.exceptions import ValidationError
from ..models_config import get_model_for_provider


class UnifiedAIClient:
    """ç»Ÿä¸€çš„AIå®¢æˆ·ç«¯ï¼Œè‡ªåŠ¨é€‚é…ä¸åŒæ¨¡å‹çš„APIæ ¼å¼"""

    @staticmethod
    def format_messages_for_provider(
        messages: List[Dict[str, str]],
        provider: str
    ) -> tuple[List[Dict[str, str]], Optional[str]]:
        """
        æ ¹æ®ä¸åŒçš„æä¾›å•†æ ¼å¼åŒ–æ¶ˆæ¯

        Args:
            messages: æ ‡å‡†æ¶ˆæ¯æ ¼å¼ [{"role": "system/user/assistant", "content": "..."}]
            provider: æä¾›å•†åç§°

        Returns:
            (formatted_messages, system_prompt) å…ƒç»„
        """
        system_prompt = None
        formatted_messages = []

        for msg in messages:
            if msg["role"] == "system":
                system_prompt = msg["content"]
                # OpenAIå’ŒGPT-4.1æ”¯æŒsystemæ¶ˆæ¯åœ¨messagesä¸­
                if provider == "openai":
                    formatted_messages.append(msg)
                # Claudeéœ€è¦å•ç‹¬çš„systemå‚æ•°
                # Geminiä¹Ÿéœ€è¦ç‰¹æ®Šå¤„ç†
            else:
                formatted_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        # ç¡®ä¿è‡³å°‘æœ‰ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        if not any(msg["role"] == "user" for msg in formatted_messages):
            if formatted_messages and formatted_messages[-1]["role"] == "assistant":
                # å¦‚æœæœ€åä¸€æ¡æ˜¯assistantæ¶ˆæ¯ï¼Œæ·»åŠ ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
                formatted_messages.append({"role": "user", "content": "Continue"})
            elif not formatted_messages:
                # å¦‚æœæ²¡æœ‰ä»»ä½•æ¶ˆæ¯ï¼Œæ·»åŠ é»˜è®¤ç”¨æˆ·æ¶ˆæ¯
                formatted_messages.append({"role": "user", "content": "Hello"})

        return formatted_messages, system_prompt

    @classmethod
    async def call(
        cls,
        user: User,
        messages: List[Dict[str, str]],
        provider: str = None,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        **kwargs
    ) -> str:
        """
        ç»Ÿä¸€çš„AIè°ƒç”¨æ¥å£

        Args:
            user: ç”¨æˆ·å¯¹è±¡
            messages: ç»Ÿä¸€çš„æ¶ˆæ¯æ ¼å¼
            provider: æŒ‡å®šæä¾›å•† (openai/claude/gemini)
            model: æŒ‡å®šæ¨¡å‹
            temperature: æ¸©åº¦å‚æ•° (0.0-1.0)
            max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
            **kwargs: å…¶ä»–ç‰¹å®šäºæ¨¡å‹çš„å‚æ•°

        Returns:
            AIç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        # è‡ªåŠ¨é€‰æ‹©æä¾›å•†
        if not provider:
            provider = cls._determine_provider(user)

        # è·å–é»˜è®¤æ¨¡å‹
        if not model:
            model = get_model_for_provider(provider)

        # DEBUG: Print what we're using
        print(f"ğŸ” UnifiedAI Debug: provider={provider}, model={model}, temperature={temperature}")

        # æ ¼å¼åŒ–æ¶ˆæ¯
        formatted_messages, system_prompt = cls.format_messages_for_provider(messages, provider)

        # è°ƒç”¨å¯¹åº”çš„API
        if provider == "openai":
            return cls._call_openai(
                user, model, formatted_messages, system_prompt,
                temperature, max_tokens, **kwargs
            )
        elif provider == "claude":
            print(f"ğŸ” Calling _call_claude with model={model}, temperature={temperature}")
            return cls._call_claude(
                user, model, formatted_messages, system_prompt,
                temperature, max_tokens, **kwargs
            )
        elif provider == "gemini":
            return cls._call_gemini(
                user, model, formatted_messages, system_prompt,
                temperature, max_tokens, **kwargs
            )
        else:
            raise ValidationError(f"Unsupported provider: {provider}")

    @staticmethod
    def _call_openai(
        user: User,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨OpenAI API"""
        if not user.openaiKey:
            raise ValidationError("OpenAI API key not configured")

        api_key = decrypt(user.openaiKey)
        client = openai.OpenAI(api_key=api_key)

        # GPT-5ç³»åˆ—åªæ”¯æŒtemperature=1.0
        if model.startswith("gpt-5"):
            print(f"ğŸ” GPT-5 detected, overriding temperature from {temperature} to 1.0")
            temperature = 1.0

        # GPT-5ç³»åˆ—çš„ç‰¹æ®Šå¤„ç†
        if model.startswith("gpt-5"):
            # æå–ç”¨æˆ·æ¶ˆæ¯
            user_message = ""
            for msg in messages:
                if msg["role"] == "user":
                    user_message = msg["content"]
                    break

            # å¦‚æœæœ‰system promptï¼Œæ·»åŠ åˆ°ç”¨æˆ·æ¶ˆæ¯å‰
            if system_prompt:
                user_message = f"{system_prompt}\n\n{user_message}"

            # å°è¯•ä½¿ç”¨æ–°çš„responses API
            try:
                response = client.responses.create(
                    model=model,
                    input=user_message,
                    reasoning=kwargs.get("reasoning", {"effort": "medium"}),
                    text=kwargs.get("text", {"verbosity": "medium"})
                )
                return response.output_text or ""
            except (AttributeError, Exception):
                # å›é€€åˆ°æ ‡å‡†API - GPT-5 ä½¿ç”¨ max_completion_tokens
                response = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_completion_tokens=max_tokens
                )
                return response.choices[0].message.content or ""

        # GPT-4.1å’Œå…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†API
        else:
            params = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
            }

            # GPT-4o ç³»åˆ—ä¹Ÿä½¿ç”¨ max_completion_tokens
            if "gpt-4o" in model or model.startswith("gpt-4-"):
                params["max_completion_tokens"] = max_tokens
            else:
                params["max_tokens"] = max_tokens

            # æ·»åŠ é¢å¤–å‚æ•°
            for key in ["top_p", "frequency_penalty", "presence_penalty"]:
                if key in kwargs:
                    params[key] = kwargs[key]

            response = client.chat.completions.create(**params)
            return response.choices[0].message.content or ""

    @staticmethod
    def _call_claude(
        user: User,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨Claude API"""
        if not user.claudeKey:
            raise ValidationError("Claude API key not configured")

        api_key = decrypt(user.claudeKey)
        client = anthropic.Anthropic(api_key=api_key)

        # Claude 4 ç³»åˆ—æ¨¡å‹åªæ”¯æŒ temperature=1.0
        # æ£€æŸ¥æ˜¯å¦ä¸º Claude 4.x æ¨¡å‹ (sonnet-4, opus-4)
        print(f"ğŸ” _call_claude: model={model}, temperature BEFORE check={temperature}")
        if "sonnet-4" in model or "opus-4" in model:
            print(f"âœ… Detected Claude 4 model, overriding temperature to 1.0")
            temperature = 1.0
        print(f"ğŸ” _call_claude: temperature AFTER check={temperature}")

        # æ„å»ºå‚æ•°
        params = {
            "model": model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature
        }
        print(f"ğŸ” Final params to Claude API: {params}")

        # æ·»åŠ system promptï¼ˆå¦‚æœæœ‰ï¼‰
        if system_prompt:
            params["system"] = system_prompt

        # æ·»åŠ é¢å¤–å‚æ•°
        for key in ["top_p", "top_k"]:
            if key in kwargs:
                params[key] = kwargs[key]

        response = client.messages.create(**params)

        # æå–å“åº”æ–‡æœ¬
        if response.content and len(response.content) > 0:
            return response.content[0].text
        return ""

    @staticmethod
    def _call_gemini(
        user: User,
        model: str,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> str:
        """è°ƒç”¨Gemini APIï¼ˆå¾…å®ç°ï¼‰"""
        if not user.geminiKey:
            raise ValidationError("Gemini API key not configured")

        # å¾…å®ç°Gemini APIè°ƒç”¨
        raise ValidationError("Gemini support is not yet implemented")

    @staticmethod
    def _determine_provider(user: User) -> str:
        """æ ¹æ®ç”¨æˆ·é…ç½®è‡ªåŠ¨é€‰æ‹©æä¾›å•†"""
        # ä¼˜å…ˆçº§ï¼šOpenAI > Claude > Gemini
        if user.openaiKey:
            return "openai"
        elif user.claudeKey:
            return "claude"
        elif user.geminiKey:
            return "gemini"
        else:
            raise ValidationError("No AI API keys configured")


# ä¾¿æ·å‡½æ•°
async def unified_ai_call(
    user: User,
    prompt: str,
    system_prompt: str = None,
    provider: str = None,
    model: str = None,
    **kwargs
) -> str:
    """
    ç®€åŒ–çš„AIè°ƒç”¨æ¥å£

    Args:
        user: ç”¨æˆ·å¯¹è±¡
        prompt: ç”¨æˆ·è¾“å…¥
        system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
        provider: æŒ‡å®šæä¾›å•†ï¼ˆå¯é€‰ï¼‰
        model: æŒ‡å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        AIå“åº”æ–‡æœ¬
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    messages.append({"role": "user", "content": prompt})

    return await UnifiedAIClient.call(
        user=user,
        messages=messages,
        provider=provider,
        model=model,
        **kwargs
    )