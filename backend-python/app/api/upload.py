from fastapi import APIRouter, Depends, UploadFile, File, HTTPException
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session
import os
import uuid
import PyPDF2
import io
from PIL import Image
import httpx
from bs4 import BeautifulSoup
import html2text
from pydantic import BaseModel
import markdown

from ..database import get_db, SessionLocal
from ..dependencies import get_current_user_db
from ..config import settings
from ..utils.exceptions import HTTPValidationError
from ..models.article import Article
from ..models.agent import Agent
from datetime import datetime

router = APIRouter()


class FileUploadResponse:
    def __init__(self, file_info: dict):
        self.success = True
        self.file = file_info


class FileParseResponse:
    def __init__(self, content: str, word_count: int):
        self.success = True
        self.content = content
        self.word_count = word_count


class URLFetchRequest(BaseModel):
    url: str


# ç®€åŒ–çš„æ–‡ä»¶å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
uploaded_files = {}


@router.post("/file")
async def upload_file(
    file: UploadFile = File(...),
    current_user = Depends(get_current_user_db)
):
    """ä¸Šä¼ æ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = {'.pdf', '.md', '.txt', '.doc', '.docx'}
    file_ext = os.path.splitext(file.filename or "")[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPValidationError("Unsupported file type")
    
    # éªŒè¯æ–‡ä»¶å¤§å°
    file_size = 0
    content = await file.read()
    file_size = len(content)
    
    if file_size > settings.max_file_size:
        raise HTTPValidationError("File too large")
    
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_path = os.path.join(settings.upload_dir, f"{file_id}{file_ext}")
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as f:
            f.write(content)
        
        # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
        file_info = {
            "id": file_id,
            "originalName": file.filename,
            "size": file_size,
            "type": file.content_type,
            "path": file_path,
            "userId": current_user.id
        }
        
        uploaded_files[file_id] = file_info
        
        return FileUploadResponse(file_info).__dict__
        
    except Exception as e:
        raise HTTPValidationError(f"File upload failed: {str(e)}")


@router.post("/parse")
async def parse_file(
    request: dict,
    current_user = Depends(get_current_user_db)
):
    """è§£ææ–‡ä»¶å†…å®¹"""
    
    file_id = request.get("fileId")
    if not file_id or file_id not in uploaded_files:
        raise HTTPValidationError("File not found")
    
    file_info = uploaded_files[file_id]
    
    # éªŒè¯æ–‡ä»¶å±äºå½“å‰ç”¨æˆ·
    if file_info["userId"] != current_user.id:
        raise HTTPValidationError("Access denied")
    
    try:
        file_path = file_info["path"]
        file_ext = os.path.splitext(file_path)[1].lower()
        
        content = ""
        
        if file_ext == '.pdf':
            # è§£æPDF
            with open(file_path, 'rb') as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"
        
        elif file_ext in ['.txt', '.md']:
            # è§£ææ–‡æœ¬æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        
        else:
            # å…¶ä»–æ ¼å¼æš‚ä¸æ”¯æŒè¯¦ç»†è§£æ
            content = f"æ–‡ä»¶ {file_info['originalName']} å·²ä¸Šä¼ ï¼Œä½†æš‚ä¸æ”¯æŒè‡ªåŠ¨è§£ææ­¤æ ¼å¼ã€‚"
        
        word_count = len(content.replace(' ', '').replace('\n', ''))
        
        return FileParseResponse(content, word_count).__dict__
        
    except Exception as e:
        raise HTTPValidationError(f"File parsing failed: {str(e)}")


@router.delete("/{file_id}")
async def delete_file(
    file_id: str,
    current_user = Depends(get_current_user_db)
):
    """åˆ é™¤æ–‡ä»¶"""
    
    if file_id not in uploaded_files:
        raise HTTPValidationError("File not found")
    
    file_info = uploaded_files[file_id]
    
    # éªŒè¯æ–‡ä»¶å±äºå½“å‰ç”¨æˆ·
    if file_info["userId"] != current_user.id:
        raise HTTPValidationError("Access denied")
    
    try:
        # åˆ é™¤ç‰©ç†æ–‡ä»¶
        if os.path.exists(file_info["path"]):
            os.remove(file_info["path"])
        
        # åˆ é™¤è®°å½•
        del uploaded_files[file_id]
        
        return {"success": True}
        
    except Exception as e:
        raise HTTPValidationError(f"File deletion failed: {str(e)}")


@router.post("/image")
async def upload_image(
    file: UploadFile = File(...),
    current_user = Depends(get_current_user_db)
):
    """ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶"""
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    allowed_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg'}
    file_ext = os.path.splitext(file.filename or "")[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPValidationError("Unsupported image type")
    
    # éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆå›¾ç‰‡é™åˆ¶5MBï¼‰
    content = await file.read()
    file_size = len(content)
    
    max_image_size = 5 * 1024 * 1024  # 5MB
    if file_size > max_image_size:
        raise HTTPValidationError("Image file too large (max 5MB)")
    
    try:
        # åˆ›å»ºç”¨æˆ·ä¸“ç”¨çš„å›¾ç‰‡ç›®å½•
        user_image_dir = os.path.join(settings.upload_dir, "images", str(current_user.id))
        os.makedirs(user_image_dir, exist_ok=True)
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        filename = f"{file_id}{file_ext}"
        file_path = os.path.join(user_image_dir, filename)
        
        # å¦‚æœæ˜¯å›¾ç‰‡æ–‡ä»¶ä¸”éSVGï¼Œè¿›è¡Œå‹ç¼©ä¼˜åŒ–
        if file_ext != '.svg':
            try:
                # ä½¿ç”¨PILä¼˜åŒ–å›¾ç‰‡
                image = Image.open(io.BytesIO(content))
                
                # å¦‚æœå›¾ç‰‡è¿‡å¤§ï¼Œè¿›è¡Œç­‰æ¯”ç¼©æ”¾
                max_size = (1920, 1080)  # æœ€å¤§å°ºå¯¸
                if image.size[0] > max_size[0] or image.size[1] > max_size[1]:
                    image.thumbnail(max_size, Image.Resampling.LANCZOS)
                
                # ä¿å­˜ä¼˜åŒ–åçš„å›¾ç‰‡
                if file_ext in ['.jpg', '.jpeg']:
                    image = image.convert('RGB')  # JPEGä¸æ”¯æŒé€æ˜åº¦
                    image.save(file_path, 'JPEG', quality=85, optimize=True)
                elif file_ext == '.png':
                    image.save(file_path, 'PNG', optimize=True)
                elif file_ext == '.webp':
                    image.save(file_path, 'WEBP', quality=85, optimize=True)
                else:
                    # å…¶ä»–æ ¼å¼ç›´æ¥ä¿å­˜åŸæ–‡ä»¶
                    with open(file_path, "wb") as f:
                        f.write(content)
            except Exception as img_error:
                print(f"Image optimization failed: {img_error}")
                # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œç›´æ¥ä¿å­˜åŸæ–‡ä»¶
                with open(file_path, "wb") as f:
                    f.write(content)
        else:
            # SVGæ–‡ä»¶ç›´æ¥ä¿å­˜
            with open(file_path, "wb") as f:
                f.write(content)
        
        # ç”Ÿæˆå¯è®¿é—®çš„URL
        image_url = f"/api/upload/images/{current_user.id}/{filename}"
        
        # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
        file_info = {
            "id": file_id,
            "originalName": file.filename,
            "size": os.path.getsize(file_path),  # ä½¿ç”¨å®é™…ä¿å­˜åçš„æ–‡ä»¶å¤§å°
            "type": file.content_type,
            "path": file_path,
            "url": image_url,  # å¯è®¿é—®çš„URL
            "userId": current_user.id
        }
        
        uploaded_files[file_id] = file_info
        
        return FileUploadResponse(file_info).__dict__
        
    except Exception as e:
        raise HTTPValidationError(f"Image upload failed: {str(e)}")


@router.get("/images/{user_id}/{filename}")
async def get_image(user_id: str, filename: str):
    """è·å–ç”¨æˆ·ä¸Šä¼ çš„å›¾ç‰‡"""

    # æ„å»ºæ–‡ä»¶è·¯å¾„
    file_path = os.path.join(settings.upload_dir, "images", user_id, filename)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="Image not found")

    # è¿”å›æ–‡ä»¶
    return FileResponse(file_path)


@router.post("/fetch-url")
async def fetch_url_content(
    request: URLFetchRequest,
    current_user = Depends(get_current_user_db)
):
    """ä»URLæŠ“å–ç½‘é¡µå†…å®¹å¹¶è½¬æ¢ä¸ºMarkdown"""

    try:
        print(f"ğŸŒ URL Import Request: {request.url}")
        # éªŒè¯URLæ ¼å¼
        url = request.url.strip()
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        print(f"ğŸ“ Normalized URL: {url}")

        # å‘é€HTTPè¯·æ±‚è·å–ç½‘é¡µå†…å®¹
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(
                url,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
                }
            )
            response.raise_for_status()

        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # åªç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾,ä¿ç•™å…¶ä»–å†…å®¹
        for script in soup(['script', 'style']):
            script.decompose()

        # å°†ç›¸å¯¹URLè½¬æ¢ä¸ºç»å¯¹URL,å¹¶ç§»é™¤æ— æ•ˆé“¾æ¥
        from urllib.parse import urljoin
        for img in soup.find_all('img'):
            if img.get('src'):
                img['src'] = urljoin(url, img['src'])
            elif img.get('data-src'):  # æœ‰äº›ç½‘ç«™ä½¿ç”¨æ‡’åŠ è½½
                img['src'] = urljoin(url, img['data-src'])

        # å¤„ç†é“¾æ¥
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                # è·³è¿‡JavaScripté“¾æ¥å’Œé”šç‚¹
                if href.startswith(('javascript:', '#', 'mailto:')):
                    # å°†é“¾æ¥è½¬æ¢ä¸ºçº¯æ–‡æœ¬
                    link.replace_with(link.get_text())
                else:
                    link['href'] = urljoin(url, href)
            else:
                # å¦‚æœæ²¡æœ‰href,å°†é“¾æ¥è½¬æ¢ä¸ºçº¯æ–‡æœ¬
                link.replace_with(link.get_text())

        # å°è¯•æå–ä¸»è¦å†…å®¹åŒºåŸŸ
        main_content = None
        for selector in ['article', 'main', '[role="main"]', '.post-content', '.article-content', '.entry-content']:
            main_content = soup.select_one(selector)
            if main_content:
                print(f"ğŸ“° Found main content with selector: {selector}")
                break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ,ä½¿ç”¨æ•´ä¸ªbody
        if not main_content:
            main_content = soup.find('body') or soup
            print(f"ğŸ“° Using body or full document")

        # è½¬æ¢ä¸ºMarkdown,ä¿ç•™å›¾ç‰‡å’Œé“¾æ¥
        h = html2text.HTML2Text()
        h.ignore_links = False
        h.ignore_images = False  # ä¿ç•™å›¾ç‰‡
        h.ignore_emphasis = False
        h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
        h.default_image_alt = 'Image'  # ä¸ºæ²¡æœ‰altçš„å›¾ç‰‡æä¾›é»˜è®¤æ–‡æœ¬
        h.images_to_alt = False  # ä¸è¦ç”¨altæ›¿æ¢å›¾ç‰‡,ä¿ç•™å®Œæ•´çš„markdownå›¾ç‰‡è¯­æ³•
        h.protect_links = True  # ä¿æŠ¤é“¾æ¥æ ¼å¼
        h.wrap_links = False  # ä¸æ¢è¡Œé“¾æ¥
        h.unicode_snob = True  # ä½¿ç”¨unicodeå­—ç¬¦
        h.skip_internal_links = False  # ä¿ç•™å†…éƒ¨é“¾æ¥

        markdown_content = h.handle(str(main_content))

        # æ¸…ç†ç©ºé“¾æ¥å’Œæ— æ•ˆæ ¼å¼
        import re
        # ç§»é™¤ç©ºé“¾æ¥ [text]() æˆ– [text](#) æˆ– [text](javascript:...)
        markdown_content = re.sub(r'\[([^\]]+)\]\(\s*\)', r'\1', markdown_content)
        markdown_content = re.sub(r'\[([^\]]+)\]\(#\)', r'\1', markdown_content)
        markdown_content = re.sub(r'\[([^\]]+)\]\(javascript:[^\)]*\)', r'\1', markdown_content)

        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        lines = markdown_content.split('\n')
        cleaned_lines = []
        prev_empty = False
        for line in lines:
            if line.strip():
                cleaned_lines.append(line)
                prev_empty = False
            elif not prev_empty:
                cleaned_lines.append('')
                prev_empty = True

        markdown_text = '\n'.join(cleaned_lines).strip()

        # å°† Markdown è½¬æ¢ä¸º HTMLï¼ˆç”¨äºç¼–è¾‘å™¨æ¸²æŸ“ï¼‰
        html_content = markdown.markdown(
            markdown_text,
            extensions=['extra', 'codehilite', 'tables', 'toc', 'nl2br']
        )

        content = html_content

        # è·å–ç½‘é¡µæ ‡é¢˜
        title = soup.find('title')
        title_text = title.get_text().strip() if title else url

        # è®¡ç®—å­—æ•°
        word_count = len(markdown_text.replace(' ', '').replace('\n', ''))

        # ç”Ÿæˆæ–‡ä»¶IDå¹¶ä¿å­˜
        file_id = str(uuid.uuid4())
        file_path = os.path.join(settings.upload_dir, f"{file_id}.md")

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# {title_text}\n\n")
            f.write(f"**æ¥æº:** {url}\n\n")
            f.write("---\n\n")
            f.write(content)

        # å­˜å‚¨æ–‡ä»¶ä¿¡æ¯
        file_size = os.path.getsize(file_path)
        file_info = {
            "id": file_id,
            "originalName": f"{title_text}.md",
            "size": file_size,
            "type": "text/markdown",
            "path": file_path,
            "userId": current_user.id,
            "url": url,
            "title": title_text
        }

        uploaded_files[file_id] = file_info

        # åˆ›å»ºæ–‡ç« è®°å½•
        db = SessionLocal()
        try:
            # è·å–ç”¨æˆ·çš„é»˜è®¤ Agent æˆ–ç¬¬ä¸€ä¸ªå¯ç”¨ Agent
            default_agent = db.query(Agent).filter(
                Agent.userId == current_user.id,
                Agent.isDefault == True
            ).first()

            if not default_agent:
                default_agent = db.query(Agent).filter(
                    Agent.userId == current_user.id
                ).first()

            if not default_agent:
                raise HTTPValidationError("æœªæ‰¾åˆ°å¯ç”¨çš„ Agentï¼Œè¯·å…ˆåˆ›å»ºä¸€ä¸ª Agent")

            print(f"ğŸ“ Using agent: {default_agent.id} - {default_agent.name}")

            # åˆ›å»ºæ–‡ç« 
            article = Article(
                userId=current_user.id,
                agentId=default_agent.id,
                title=title_text,
                content=content,
                summary=f"ä»URLå¯¼å…¥: {url}",
                publishStatus="draft",
                sourceFiles=url
            )
            db.add(article)
            db.commit()
            db.refresh(article)

            print(f"âœ… Article created: {article.id} - {article.title}")

            return {
                "success": True,
                "file": file_info,
                "content": content,
                "word_count": word_count,
                "title": title_text,
                "article": {
                    "id": article.id,
                    "title": article.title,
                    "status": "imported",
                    "created_at": article.createdAt.isoformat() if article.createdAt else datetime.utcnow().isoformat()
                }
            }
        finally:
            db.close()

    except httpx.HTTPError as e:
        print(f"âŒ HTTP Error: {str(e)}")
        raise HTTPValidationError(f"æ— æ³•è®¿é—®URLï¼š{str(e)}")
    except Exception as e:
        print(f"âŒ Unexpected Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPValidationError(f"å†…å®¹æŠ“å–å¤±è´¥ï¼š{str(e)}")